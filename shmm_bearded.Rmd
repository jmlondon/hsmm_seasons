---
title: "Estimating Seasonal Behavior States from Biologging Sensor Data"
subtitle: "An Initial Example with Bearded Seals in the Bering Sea"
draft: true
author:
- name: Josh M. London
  affiliation: 1
- name: Devin S. Johnson
  affiliation: 1
- name: Paul B. Conn
  affiliation: 1
- name: Brett T. McClintock
  affiliation: 1
address:
- code: 1
  address: Alaska Fisheries Science Center, NOAA Fisheries, Seattle, Washington, United States 
  email: josh.london@noaa.gov
  orcid: orcid.org/0000-0002-3647-5046
date: "`r format(Sys.time(), '%d %B, %Y')`"
disclaimer: >
  The scientific results and conclusions, as well as any views or opinions 
  expressed herein, are those of the author(s) and do not necessarily reflect 
  those of NOAA or the Department of Commerce.
abstract: >
  The seasonal timing of key, annual life history events is an important component of many species' ecology. Seasonal periods important to marine mammals often do not align well with typical labels (i.e., spring, summer, winter, fall). The timing of key life history events is well documented only for species found in accessible rookeries or breeding areas. Our knowledge of seasonal timing for species widely dispersed in inaccessible or remote habitats is poor. Here, we employed data from biologging sensors and new statistical modeling to identify and estimate timing of seasonal states for ribbon (n=82) and spotted seals (n=46) in the Bering Sea. These seals are reliant on the seasonal sea ice for pupping, nursing, breeding and molting and these seasons can be characterized by more time spent hauled out on ice and by changes in dive behavior. We are especially interested in the pupping-breeding-molting season, but also used this approach to identify seasonal structure in the non-breeding period. Seasonal periods were treated as separate behavior states that correspond to a hidden Markov process. Hidden Markov models (HMM) are commonly used to estimate behavior states (e.g., foraging, resting, transit) from telemetry data. Typical HMMs, however, have no temporal memory of state assignments and would likely not capture seasonal level states. To address this, we applied a new approximation to a hidden semi-Markov model and specified the transition matrix for the states to mimic the sequential timing of seasons. Dive and haul-out behavior from biologgers were used to estimate these states. The timing and extent of sea ice in the Bering Sea is predicted to change dramatically over the next 50 years and we anticipate ribbon and spotted seals might adjust the timing of these life history events in response to those changes.

output:
  html_document:
    uswebr::noaa_report
---

```{r global-options, include=FALSE}
# set global chunk options
library(knitr)
options(width=80)
opts_chunk$set(dev="png",
               dev.args=list(type="cairo",antialias="none"),
               dpi=192,warning=FALSE, message=FALSE)
```


## Introduction

Since 2006, over 128 ribbon and spotted seals have been captured and outfitted with bio-logging devices that measure hourly wet/dry proportions. In addition, seven adult bearded seals were released with bio-logging devices. The critical life history events are all associated with increased haul-out behavior. Hidden Markov models require a geometrically distributed sojourn time in a given state. Hidden semi-Markov models allow an arbitrary sojourn distribution. In other words, the Hidden semi-Markov approach allows for estimation of states that persist longer than the observation interval. Oâ€™Connell et al (2010) applied Hidden semi-Markov models to the estrus detection in dairy cows and developed the mhsmm library for R to support similar analyses.

We expect ribbon seals to have at least 3 distinct annual states: an open water or pelagic period characterized by limited to no haul-out behavior, pupping/breeding/molting period characterized by increased haul-out frequency and duration, a transition period leading up to the pupping/breeding/molting period with increasing haul-out behavior as sea ice forms and stabilizes within the Bering Sea. These periods are generally known to correspond to July-November, April-June and December-March, but the specifics are unknown. Spotted seals should mirror some of these patterns although the open water period will be likely replaced with a near-shore period. BeardedWhile, initially, the focus will be on a single observation channel (proportion dry per hour), a multivariate approach would allow inclusion of dive behavior. Additionally, we would expect variability between age and sex classes and for key covariates (e.g. sea-ice concentration or distance to sea-ice edge) to influence state assignment.


## Methods and Analysis

We will start by loading the bearded seal data from the `kotzeb0912` package along with other packages we'll need for the data munging and analysis.

```{r load-eb-data}
library(kotzeb0912)
library(sp)
library(rgdal)
library(trip)
library(crawl)
library(argosfilter)
library(parallel)
library(dplyr)
library(lubridate)
```

### Model Seal Movement

The first step will be to load in the location data and run these data through the `crawl` package to model seal movement and estimate locations every 6 hours.

Prior to modeling the movement with `crawl`, we will use the `argosfilter` package to pass the Argos locations through a course speed filter (vmax=3.5 m/s).

```{r argos-filter}
data(kotzeb0912_locs)

locs <- dplyr::filter(kotzeb0912_locs,instr=="Mk10") %>% 
  dplyr::arrange(deployid,unique_posix)
# speedfilter using the paralell package for multi-core speed
cfilter<-mclapply(split(locs,locs$deployid),function(x) sdafilter(
  lat=x$latitude, lon=x$longitude, dtime=x$unique_posix,
  lc=x$quality, ang=-1,vmax=3.5),mc.preschedule=F,mc.cores=3)
cfilter<-do.call("c",cfilter)
cfilter<-as.vector(cfilter)
locs$filtered <- cfilter

data <- filter(locs,filtered=="not", !is.na(error_semimajor_axis)) %>% arrange(deployid,unique_posix) %>% as.data.frame(.)

```

A few of the bearded seals were deployed with GPS capable tags and so we need to merge the GPS data with our Argos data.

```{r gps-data}
data(kotzeb0912_gps)

kotzeb0912_gps <- kotzeb0912_gps %>% 
  select(deployid,date_time,latitude,longitude) %>% 
  filter(!is.na(latitude)) %>% 
  rename(unique_posix=date_time) %>% 
  mutate(error_semimajor_axis = 50,error_semiminor_axis = 50, error_ellipse_orientation = 1) 

data <- data %>% bind_rows(kotzeb0912_gps) %>% 
  arrange(deployid,unique_posix)
```


```{r crawl-model, results='hide'}
data <- as.data.frame(data)
coordinates(data) = ~longitude+latitude
proj4string(data) = CRS("+proj=longlat +datum=WGS84")

data=spTransform(data, CRS("+init=epsg:3571"))

## Loop over PTT and fit CTCRW models
ids = unique(data@data$deployid)

library(doParallel)
n.cores <- detectCores()
registerDoParallel(cores=3)

model_fits <-
  foreach(i = 1:length(ids)) %dopar% {
  id_data = subset(data,deployid==ids[i])
  diag_data = model.matrix(
      ~ error_semimajor_axis + error_semiminor_axis + error_ellipse_orientation,
      id_data@data
    )[,-1]
    
    id_data@data = cbind(id_data@data, 
                         argosDiag2Cov(
                           diag_data[,1], 
                           diag_data[,2], 
                           diag_data[,3]))
    
    init = list(a = c(coordinates(id_data)[1,1],0,
                      coordinates(id_data)[1,2],0),
                P = diag(c(5000 ^ 2,10 * 3600 ^ 2, 
                           5000 ^ 2, 10 * 3600 ^ 2)))
    
  fit <- crwMLE(
      mov.model =  ~ 1,
      err.model = list(
        x =  ~ ln.sd.x - 1, 
        y =  ~ ln.sd.y - 1, 
        rho =  ~ error.corr
      ),
      data = id_data,
      Time.name = "unique_posix",
      initial.state = init,
      fixPar = c(1,1,NA,NA),
      theta = c(log(10), 3),
      initialSANN = list(maxit = 2500),
      control = list(REPORT = 10, trace = 1)
    )
    fit
  }

names(model_fits) <- ids

print(model_fits)

```

### Predict locations every 6 hours

This function predicts the regular-timed -- in this case, 6-hourly -- locations along the movement path using the posterior mean and variance of the track. 

```{r message=FALSE}
predData <- foreach(i = 1:length(model_fits)) %dopar% {

  model_fits[[i]]$data$unique_posix <- lubridate::with_tz(
    model_fits[[i]]$data$unique_posix,"GMT")
  predTimes <- seq(
    lubridate::ceiling_date(min(model_fits[[i]]$data$unique_posix),"day"),
    lubridate::floor_date(max(model_fits[[i]]$data$unique_posix),"day"),
    "6 hours")
  tmp = crwPredict(model_fits[[i]], predTime=predTimes)
}


predData <- dplyr::bind_rows(predData) %>% 
  filter(locType=="p")

predData$predTimes <- intToPOSIX(predData$TimeNum)

```


### Merge predicted locations with haul-out data

The haul-out behavior timelines are provided as hour percent-dry values. We want to group these values into 6-hour blocks that are centered on our 6-hourly predictions. To accomplish this, we will setup a grouping column that assigns a unique integer to each 6 hour period.

```{r load-timelines}
data(kotzeb0912_timelines)

kotzeb0912_timelines <- kotzeb0912_timelines %>% 
  filter(deployid %in% unique(predData$deployid))
```

```{r create-6hr-groups}
ids <- unique(kotzeb0912_timelines$deployid)
timelineData <- foreach(i = 1:length(ids)) %dopar% {
  t_dat <- kotzeb0912_timelines %>% 
    filter(deployid == ids[i]) %>% 
    arrange(deployid,datadatetime)
  t_seq <- data.frame(datadatetime = seq(
    lubridate::ceiling_date(min(t_dat$datadatetime),"day"),
    lubridate::floor_date(max(t_dat$datadatetime),"day"),
    "1 hour"),deployid=ids[i])
  t_dat <- merge(t_dat,t_seq,all=TRUE)
  rep_len <- ceiling((nrow(t_dat)-3)/6)
  t_group <- c(rep(1,3),rep(2:rep_len,each=6))
  t_group <- data.frame(group=t_group[1:nrow(t_dat)])
  t_dat <-cbind(t_dat,t_group) %>% filter(!is.na(group))
}

timelineData <- dplyr::bind_rows(timelineData) %>% 
  group_by(deployid,group) %>% 
  summarise(percent_dry=mean(percent_dry,na.rm=TRUE),
            datadatetime=ceiling_date(median(datadatetime),"hour")) %>% 
  select(deployid,datadatetime,percent_dry)
```

```{r merge-timeline-with-locs}
predData <- dplyr::inner_join(predData,
                              timelineData,
                              by = c("predTimes" = "datadatetime", 
                                     "deployid")) %>%
  tbl_df() %>%
  select(deployid,predTimes,mu.x,mu.y,percent_dry) %>% 
  arrange(deployid, predTimes) %>% 
  rename(datadatetime=predTimes)
```

### Merge predicted locations with dive behavior

Dive behavior data are transmitted as dive histograms that represent the distribution of dives across predetermined depth bins for a given 6-hour period. To simplify things, we will sum the number of dives across all bins less than 10m. This should represent the most likely number of foraging dives over a 6 hour period.

```{r load-dive-depths}
data(kotzeb0912_depths)

diveData <- kotzeb0912_depths %>% 
  filter(limits != "10.000000") %>% 
  filter(num_dives < 80) %>% 
  mutate(datadatetime = datadatetime + lubridate::hours(3)) %>% 
  group_by(deployid,datadatetime) %>% 
  summarise(num_dives=sum(num_dives,na.rm=TRUE))

predData <- dplyr::left_join(predData,
                             diveData) %>% 
  tbl_df() %>% 
  arrange(deployid,datadatetime)
```

### Calculate x and y displacement for each step

We'll use the dplyr::lag() function to calculate the difference in x and y when compared with the previous x and y values. This will give us a measure of northing and easting movement.

```{r xy-displacement}
predData <- predData %>% 
  group_by(deployid) %>% 
  mutate(x_disp = order_by(datadatetime, mu.x - lag(mu.x)), 
         y_disp = order_by(datadatetime, mu.y - lag(mu.y))
  ) %>% 
  filter(!is.na(x_disp) | !is.na(y_disp))
```

and the we can use a custom function to calculate the compass bearing between those two points

```{r compass-bearing}
anglefun <- function(xx,yy,bearing=TRUE,as.deg=FALSE){
  ## calculates the compass bearing of the line between two points
  ## xx and yy are the differences in x and y coordinates between two points
  ## Options:
  ## bearing = FALSE returns +/- pi instead of 0:2*pi
  ## as.deg = TRUE returns degrees instead of radians
  c = 1
  if (as.deg){
    c = 180/pi
  }
  
  b<-sign(xx)
  b[b==0]<-1  #corrects for the fact that sign(0) == 0
  tempangle = b*(yy<0)*pi+atan(xx/yy)
  if(bearing){
    #return a compass bearing 0 to 2pi
    #if bearing==FALSE then a heading (+/- pi) is returned
    tempangle[tempangle<0]<-tempangle[tempangle<0]+2*pi
  }
  return(tempangle*c)
}

predData <- predData %>% 
  mutate(bearing = anglefun(x_disp,y_disp,as.deg=TRUE))
```

### Add Day-of-Year and Deployment-Day for aligning deployments

Since these deployments occurred over multiple years, we will want a convenient way to align the deployments. The simplest way to do this is to use day-of-year integers and pick start/end values that provide a sensible amount of overlap between the 7 deployments.

First, we'll look at the start dates for each deployment

```{r deploy-starts}
predData %>% group_by(deployid) %>% 
  filter(row_number(datadatetime) == 1)
```

And, now, we'll look at the end dates for each deployment

```{r deploy-end}
predData %>% group_by(deployid) %>% 
  filter(row_number(datadatetime) == max(row_number(datadatetime)))
```

From this information is seems sensible to have all of the deployments start on 05 July (186th day of the year). 

```{r limit-by-doy}
predData <- predData %>% 
  mutate(deploy_day = ifelse(lubridate::yday(datadatetime)<150, 
                      lubridate::yday(datadatetime)+365 +
                        lubridate::hour(datadatetime)/24,
                      lubridate::yday(datadatetime) +
                        lubridate::hour(datadatetime)/24)) %>% 
  mutate(corrupt = ifelse(deployid == "EB2009_3001_06A1332" & 
                            datadatetime > lubridate::ymd_hms("2010-01-28 03:00:00"),
                          TRUE,FALSE
                          )) %>% 
  filter(abs(y_disp) < 100000, abs(x_disp) < 100000) %>% 
  filter(!corrupt)

save(predData,file='predData.rda')
```

```{r plot-1, fig.height = 9}
library(ggplot2)
library(uswebr)

p1 <- ggplot(predData, aes(x=deploy_day, y=percent_dry)) + geom_point(alpha=0.2) + facet_grid(deployid ~ .) + 
  usweb_theme()
p1
```

```{r plot-2, fig.height = 9}
p2 <- ggplot(predData, aes(x=deploy_day, y=num_dives)) + 
  geom_point(alpha=0.2) + facet_grid(deployid ~ .) + 
  usweb_theme()
p2
```

```{r plot-3, fig.height = 9}
predData <- predData %>% mutate(direction = ifelse(
  y_disp >= 0, "north","south"))

ggplot(predData,aes(x=deploy_day, y=y_disp/1000)) +
  geom_bar(aes(color=factor(direction)),alpha=0.2,stat="identity") +
  facet_grid(deployid ~ .) + usweb_theme()
```

## Results



## Discussion



## References
